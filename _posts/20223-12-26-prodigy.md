---
title: "[Paper] PRODIGY: Enabling In-context Learning Over Graphs"
layout: post
post-image: "/assets/posts/prodigy/thumb.png"
description: 
tags:
    Large Language Models
    Knowledge Graph
    In-context Learning
---
### Abstract
- The first pretraining framework that enables in-context learning over graphs.
- How? Suggesting the novel prompt graph representation
- Architecture based on the graph neural network.
- Can do classification task on unseen graph via ICL.
  
### Introduction

- **Primary Challenge** : How to formulate an represent node, edge, and graph level tasks over graphs with a unified task representation 

*i.e. What is an analog of natural language prompting for graph tasks?*

- **Second Challenge** : Which objective should we use for pretraining?
    - Usually, graph pretraining focused either on generalizing task or obtaining good graph encoder. 
    - However, ICL with graph needs to satisfy **Generalization** without finetuning.


- ***Prompt Graph***

![fig](/assets/posts/prodigy/1.png){: width='100%'}

 - Model is pretrained to the wide range of tasks and graphs.
 - Continue to do so out-of-the box.

### In-context Learning over Graphs
