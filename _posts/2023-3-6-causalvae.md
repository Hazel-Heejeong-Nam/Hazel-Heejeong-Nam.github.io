---
title: "[Paper] CausalVAE : Disentangled Respresentation Learning via Neural Structural Causal Models"
layout: post
post-image: "/assets/posts/causalvae/thumb.png"
description: CVPR 2021
tags:
- Causality
- Disentangling
- Weakly-supervised learning
- Computer vision
---

### Introduction
A common assumption in DRL is that real-world observations can be generated by a finite number of independent factors. However, in reality, semantic factors are not necessarily independent.

For example, the position of the illumination source and the angle of the pendulum are causes of the position and the length of the shadow.

Therefore, this paper proposes causal disentanglement learning, which assumes that there can be causal relationships between semantic factors. This model can generate counterfactual data, which corresponds to the 'do operation', a crucial element in causality learning.

Moreover, causality enables 'what-if' questions in the model's decision-making process.

CausalVAE is a VAE-based model with an added Structural Causal Model (SCM) layer. SCM is created in the Directed Acyclic Graph space. While there have been unsupervised DRLs proposed for learning causality, the feasibility was unclear since the causal structure of latent factors had to be given as a prior.

CausalVAE, therefore, requires weak supervision. Unlike unsupervised causality learning, it allows the model to automatically learn the causal structure. The loss includes the ELBO of VAE and an acyclicity constraint to ensure 'DAGness'.

The presented model enables causal disentanglement, do operations, and justification on model identifiability.

### Related Work
Conventional: Initially, the basic encoder-decoder structure of VAEs was used, where the KL divergence in VAEs forces the learning of independent latent factors.

Later: In various VAE variants, modifications or additions to the loss term were made to ensure more definitive disentanglement.

New Approach: Criticisms arose that the assumption of independence undermines interpretability in complex situations, leading to the emergence of methods that consider complex causal relationships. However, this introduces an identifiability issue. Given the same joint distribution, an infinite number of distinct models can be produced, making them unidentifiable in unsupervised learning. Efforts have been made to enhance model identifiability and reduce model ambiguity through additional inputs, labels, etc.

Applying disentangled techniques to causality is a recent development, though not entirely novel. CausalGAN, mentioned earlier, enables do operations in an unsupervised manner but requires a prior causal graph. Models of causality including non-structured nodes have also been proposed.

### Model Description
The core of the model presented in this paper is the Causal layer. This layer transforms the encoder output, which is the state of independent exogenous factors, into causal endogenous factors. It is also referred to as the Masking layer, the reason for which is explained later in the paper. This Causal layer is crucial in enabling interventions or do operations.

![fig](/assets/posts/causalvae/1.png){: width="100%"}

Firstly, let's denote the number of factors we want to observe as n. In other words, we can think of the DAG as having n nodes. Epsilon represents the state after passing through the encoder, and upon passing through the causal layer, it results in a latent z, which is a structured representation on the DAG.



As mentioned earlier, this model is a weakly supervised model. This means that, in addition to observed data, there exists an additional input, denoted as u. In this model, u represents labels for each concept. By providing these labels, identifiability can be ensured. Moreover, u is used when generating counterfactual data.

![fig](/assets/posts/causalvae/2.png){: width="100%"}

Now, let's look at what happens in the mask layer. When Ai and z are multiplied element-wise, the output only contains information about the parents of i. I haven't seen the code, but it reminded me of the look-ahead mask in transformers. In fact, I found that there are similar papers on transformer causality learning.


### Experiments
- dataset : synthetic (Pendulum, Flow) & CelebA
- Baselines :
  - Unsupervised : CausalVAE-unsup, LadderVAE, beta-VAE
  - Supervised : ConditionVAE, CausalGAN
- Metrics : 
  - Maximal Information Coefficient
  - Total Information Coefficient