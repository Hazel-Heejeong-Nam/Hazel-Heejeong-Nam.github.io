---
title: "[Paper] Accounting for Dependencies in Deep Learning based Multiple Instance Learning for Whole Slide Imaging"
layout: post
post-image: "/assets/posts/monaimil/thumb.png"
description: MICCAI 2021
tags:
- multiple instance learning
- weakly-supervised learning
- computer vision
---


### Key points <br>
* Account for dependencies between instances by using self-attention transformer block
* Creating instance-level pseudo label and use intance-wise loss function

### MIL <br>
* Weakly supervised algrithm (patch-level annotation x, Only WSI label O)
* Why use patch in pathology image? 
   *Tumor only takes small part in WSI*
* What is a difference between classic MIL and this one
   * Classic MIL
     * no patch level label - thus inference every patch.
     * But leverage only one patch with the highest prediction probability. 

   * Attn_trans MIL
     * Use dependency among patches given by self attention, addressing efficiency issue. 

![method1](/assets/posts/monaimil/1.png){: width="100%"}
![method1](/assets/posts/monaimil/2.png){: width="100%"}
![method1](/assets/posts/monaimil/3.png){: width="100%"}
![method1](/assets/posts/monaimil/4.png){: width="100%"}


### Variants
1. Transformer MIL
   * Locate a transformer encoder block only at the end of the CNN (before average pooling)
2. Pyramid Transformer MIL
   * Attach separate transformer blocks after each of the main ResNet blocks
     * Can capture at different levels of its feature pyramid ( Capture dependencies between batches at multiple scale)

### Implementation details
* models : Resnet50 pre-trained on ImageNet
* optimizer : Adam
* lr: 3e-4 for cnn backbone, 3e-5 for transformer
* batchsize 16 and 5 fold cross validation

### Evalutation
* Metric : Accuracy, AUC, QWK(main)
  * QWK (quadratic weighted kappa) : *measures the similarity between the predictions and targets using confusion matrix, ranges of -1~1*

![method5](/assets/posts/monaimil/5.png){: width='100%'}