---
title: "[Paper] DeepObliviate : A Powerful Charm for Erasing Data REsidual Memory in Deep Neural Networks"
layout: post
post-image: "/assets/posts/deepob/thumb.png"
description: 
tags:
    Machine Unlearning
---
### Main idea
Storing intermediate models on the hard disk. 

1. quantify temporal residual memory left in stored model. 

2. retrain and decide when to terminate

3. combine retrained model and uninfluenced model. 

We retrain the intermediate models for training each block , and divide them into four areas ; unseen, deleted, affected, unaffected (where hte residual memory extinguishes. )

   $+$ backdoor verification method to guarantee

### Detail

![1](/assets/posts/deepob/1.png){: width="100%"}

#### Setup

1. The entire dataset is divided into B blocks.

2. The training process considers each block as a complete dataset for multi-epoch training.

3. At the end of training for each block, the model is saved on hardware.

4. Once all blocks are trained, there will be B models saved.
#### Unlearn

1. When data to be unlearned first appears in the d-th block, there is no need to alter blocks up to d-1. From the d-th block onwards, remove the data to be unlearned and retrain.
2. Retraining is not done until the end. According to the author's experimental findings, unlearned data will not have new influence on subsequent training (almost no effect) after a certain time (t).
3. Therefore, retraining is stopped at the point of d+t, and a subtractive model is stitched.

![1](/assets/posts/deepob/2.png){: width="70%"}


#### Recognize the ending point
Temporal residual memory

#### How to stitch
- Element wise arithmetic
![1](/assets/posts/deepob/3.png){: width="50%"}

Here, M' is the model before encountering the unlearned data block D, M_B is the pretrained model that went through all blocks, and Md+t is the model after forgetting the unlearned data.
### Conclude




  

